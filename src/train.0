/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using backend: tensorflow.compat.v1
Other supported backends: tensorflow, pytorch, jax, paddle.
paddle supports more examples now and is recommended.
WARNING:tensorflow:From /home/bogdan/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2023-11-18 23:48:56.905382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 23:48:56.936458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 23:48:56.936647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
Enable just-in-time compilation with XLA.

WARNING:tensorflow:From /home/bogdan/.local/lib/python3.8/site-packages/deepxde/nn/initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Warning: 1 points required, but 411 points sampled.
Warning: 5000 points required, but 2055000 points sampled.
Warning: 500 points required, but 1942 points sampled.
Warning: 500 points required, but 1942 points sampled.
Compiling model...
Building feed-forward neural network...
/home/bogdan/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/fnn.py:116: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.038003 s

2023-11-18 23:49:11.280032: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-18 23:49:11.280689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 23:49:11.280873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 23:49:11.280997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 23:49:11.682899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 23:49:11.683072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 23:49:11.683207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 23:49:11.683299: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2023-11-18 23:49:11.683332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8097 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:27:00.0, compute capability: 8.6
'compile' took 0.692783 s

2023-11-18 23:49:11.939706: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
Training model...

2023-11-18 23:49:12.049107: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7efc3820a2b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-11-18 23:49:12.049167: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6
2023-11-18 23:49:12.068621: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-11-18 23:49:18.627218: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Step      Train loss              Test loss               Test metric
0         [5.14e+01, 3.03e-02]    [5.14e+01, 3.03e-02]    []  
1000      [3.15e-01, 2.16e-01]    [3.15e-01, 2.16e-01]    []  
2000      [8.80e-01, 2.21e-01]    [8.80e-01, 2.21e-01]    []  
3000      [1.18e-01, 1.98e-01]    [1.18e-01, 1.98e-01]    []  
4000      [9.28e-02, 1.88e-01]    [9.28e-02, 1.88e-01]    []  
5000      [7.04e-02, 1.61e-01]    [7.04e-02, 1.61e-01]    []  
6000      [6.16e-02, 8.81e-02]    [6.16e-02, 8.81e-02]    []  
7000      [5.07e-02, 3.30e-02]    [5.07e-02, 3.30e-02]    []  
8000      [3.13e-02, 1.36e-02]    [3.13e-02, 1.36e-02]    []  
9000      [2.35e-02, 6.72e-03]    [2.35e-02, 6.72e-03]    []  
10000     [1.88e-02, 3.74e-03]    [1.88e-02, 3.74e-03]    []  
11000     [1.57e-02, 2.41e-03]    [1.57e-02, 2.41e-03]    []  
12000     [1.34e-02, 1.75e-03]    [1.34e-02, 1.75e-03]    []  
13000     [1.17e-02, 1.41e-03]    [1.17e-02, 1.41e-03]    []  
14000     [2.25e-01, 2.63e-03]    [2.25e-01, 2.63e-03]    []  
15000     [2.57e-02, 1.03e-03]    [2.57e-02, 1.03e-03]    []  
16000     [8.33e-03, 8.76e-04]    [8.33e-03, 8.76e-04]    []  
17000     [7.68e-03, 7.10e-04]    [7.68e-03, 7.10e-04]    []  
18000     [7.03e-03, 5.45e-04]    [7.03e-03, 5.45e-04]    []  
19000     [3.05e-02, 4.55e-04]    [3.05e-02, 4.55e-04]    []  
20000     [6.06e-03, 4.17e-04]    [6.06e-03, 4.17e-04]    []  
21000     [1.18e-01, 7.50e-04]    [1.18e-01, 7.50e-04]    []  
22000     [5.12e-03, 2.86e-04]    [5.12e-03, 2.86e-04]    []  
23000     [4.73e-03, 4.22e-04]    [4.73e-03, 4.22e-04]    []  
24000     [3.95e-03, 4.16e-04]    [3.95e-03, 4.16e-04]    []  
25000     [3.49e-03, 4.03e-04]    [3.49e-03, 4.03e-04]    []  
26000     [3.88e-03, 5.80e-04]    [3.88e-03, 5.80e-04]    []  
27000     [5.42e-03, 4.39e-04]    [5.42e-03, 4.39e-04]    []  
28000     [2.51e-03, 3.62e-04]    [2.51e-03, 3.62e-04]    []  
29000     [1.76e-03, 2.42e-04]    [1.76e-03, 2.42e-04]    []  
30000     [1.95e-03, 2.61e-04]    [1.95e-03, 2.61e-04]    []  
31000     [1.08e-03, 1.44e-04]    [1.08e-03, 1.44e-04]    []  
32000     [7.88e-04, 6.46e-05]    [7.88e-04, 6.46e-05]    []  
33000     [4.64e-04, 3.90e-05]    [4.64e-04, 3.90e-05]    []  
34000     [2.60e-03, 4.74e-04]    [2.60e-03, 4.74e-04]    []  
35000     [6.75e-04, 4.08e-05]    [6.75e-04, 4.08e-05]    []  
36000     [7.67e-04, 3.90e-05]    [7.67e-04, 3.90e-05]    []  
37000     [5.08e-04, 1.08e-04]    [5.08e-04, 1.08e-04]    []  
38000     [8.94e-03, 3.40e-04]    [8.94e-03, 3.40e-04]    []  
39000     [3.11e-04, 1.16e-05]    [3.11e-04, 1.16e-05]    []  
40000     [2.89e-04, 1.16e-05]    [2.89e-04, 1.16e-05]    []  
41000     [7.26e-03, 3.57e-04]    [7.26e-03, 3.57e-04]    []  
42000     [8.28e-04, 1.73e-04]    [8.28e-04, 1.73e-04]    []  
43000     [3.31e-04, 4.52e-05]    [3.31e-04, 4.52e-05]    []  
44000     [2.19e-04, 2.95e-05]    [2.19e-04, 2.95e-05]    []  
45000     [1.07e-03, 1.88e-04]    [1.07e-03, 1.88e-04]    []  
46000     [1.52e-04, 1.76e-05]    [1.52e-04, 1.76e-05]    []  
47000     [1.39e-04, 1.45e-05]    [1.39e-04, 1.45e-05]    []  
48000     [1.27e-04, 1.23e-05]    [1.27e-04, 1.23e-05]    []  
49000     [1.16e-04, 1.06e-05]    [1.16e-04, 1.06e-05]    []  
50000     [1.07e-04, 9.04e-06]    [1.07e-04, 9.04e-06]    []  
51000     [1.00e-04, 7.69e-06]    [1.00e-04, 7.69e-06]    []  
52000     [9.35e-05, 6.53e-06]    [9.35e-05, 6.53e-06]    []  
53000     [8.82e-05, 5.86e-06]    [8.82e-05, 5.86e-06]    []  
54000     [9.21e-05, 5.38e-06]    [9.21e-05, 5.38e-06]    []  
55000     [4.19e-04, 7.78e-05]    [4.19e-04, 7.78e-05]    []  
56000     [2.54e-04, 3.50e-05]    [2.54e-04, 3.50e-05]    []  
57000     [2.05e-04, 2.46e-05]    [2.05e-04, 2.46e-05]    []  
58000     [1.65e-04, 1.94e-05]    [1.65e-04, 1.94e-05]    []  
59000     [3.41e-04, 1.64e-04]    [3.41e-04, 1.64e-04]    []  
60000     [1.16e-04, 1.39e-05]    [1.16e-04, 1.39e-05]    []  
61000     [1.51e-04, 1.58e-05]    [1.51e-04, 1.58e-05]    []  
62000     [1.27e-04, 1.18e-05]    [1.27e-04, 1.18e-05]    []  
63000     [9.45e-05, 1.56e-05]    [9.45e-05, 1.56e-05]    []  
64000     [1.04e-04, 1.47e-05]    [1.04e-04, 1.47e-05]    []  
65000     [9.25e-05, 1.37e-05]    [9.25e-05, 1.37e-05]    []  
66000     [7.68e-05, 1.15e-05]    [7.68e-05, 1.15e-05]    []  
67000     [1.82e-04, 4.17e-05]    [1.82e-04, 4.17e-05]    []  
68000     [4.17e-04, 1.21e-05]    [4.17e-04, 1.21e-05]    []  
69000     [7.75e-05, 1.06e-05]    [7.75e-05, 1.06e-05]    []  
70000     [6.84e-05, 8.60e-06]    [6.84e-05, 8.60e-06]    []  
71000     [1.34e-01, 2.12e-02]    [1.34e-01, 2.12e-02]    []  
72000     [4.89e-04, 5.56e-05]    [4.89e-04, 5.56e-05]    []  
73000     [6.12e-04, 3.07e-05]    [6.12e-04, 3.07e-05]    []  
74000     [2.36e-04, 1.75e-05]    [2.36e-04, 1.75e-05]    []  
75000     [4.52e-04, 2.84e-05]    [4.52e-04, 2.84e-05]    []  
76000     [1.69e-04, 1.74e-05]    [1.69e-04, 1.74e-05]    []  
77000     [1.41e-03, 1.15e-04]    [1.41e-03, 1.15e-04]    []  
78000     [3.44e-04, 2.01e-05]    [3.44e-04, 2.01e-05]    []  
79000     [6.23e-04, 1.28e-04]    [6.23e-04, 1.28e-04]    []  
80000     [2.64e-04, 4.15e-05]    [2.64e-04, 4.15e-05]    []  
81000     [2.47e-04, 3.64e-05]    [2.47e-04, 3.64e-05]    []  
82000     [1.60e-04, 2.01e-05]    [1.60e-04, 2.01e-05]    []  
83000     [2.95e-04, 5.79e-06]    [2.95e-04, 5.79e-06]    []  
84000     [3.63e-04, 5.87e-06]    [3.63e-04, 5.87e-06]    []  
85000     [3.18e-04, 1.11e-05]    [3.18e-04, 1.11e-05]    []  
86000     [3.30e-04, 3.73e-05]    [3.30e-04, 3.73e-05]    []  
87000     [3.09e-04, 1.80e-05]    [3.09e-04, 1.80e-05]    []  
88000     [1.29e-04, 3.52e-05]    [1.29e-04, 3.52e-05]    []  
89000     [1.11e-04, 1.44e-05]    [1.11e-04, 1.44e-05]    []  
90000     [1.90e-04, 1.18e-05]    [1.90e-04, 1.18e-05]    []  
91000     [5.44e-05, 3.91e-06]    [5.44e-05, 3.91e-06]    []  
92000     [8.56e-05, 1.11e-05]    [8.56e-05, 1.11e-05]    []  
93000     [3.80e-04, 2.16e-05]    [3.80e-04, 2.16e-05]    []  
94000     [9.19e-05, 1.37e-05]    [9.19e-05, 1.37e-05]    []  
95000     [3.66e-04, 1.09e-05]    [3.66e-04, 1.09e-05]    []  
96000     [6.90e-05, 5.42e-06]    [6.90e-05, 5.42e-06]    []  
97000     [3.46e-05, 3.76e-06]    [3.46e-05, 3.76e-06]    []  
98000     [1.52e-04, 3.05e-05]    [1.52e-04, 3.05e-05]    []  
99000     [4.26e-05, 1.88e-06]    [4.26e-05, 1.88e-06]    []  
100000    [4.28e-04, 3.21e-05]    [4.28e-04, 3.21e-05]    []  

Best model at step 97000:
  train loss: 3.84e-05
  test loss: 3.84e-05
  test metric: []

'train' took 6217.732695 s

Compiling model...
'compile' took 0.174777 s

2023-11-19 01:32:49.891284: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1750] (One-time warning): Not using XLA:CPU for cluster.

If you want XLA:CPU, do one of the following:

 - set the TF_XLA_FLAGS to include "--tf_xla_cpu_global_jit", or
 - set cpu_global_jit to true on this session's OptimizerOptions, or
 - use experimental_jit_scope, or
 - use tf.function(jit_compile=True).

To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a
proper command-line flag, not via TF_XLA_FLAGS).
Training model...

Step      Train loss              Test loss               Test metric
100000    [4.28e-03, 3.21e-05]    [4.28e-03, 3.21e-05]    []  
100299    [2.15e-03, 3.58e-04]    [2.15e-03, 3.58e-04]    []  

Best model at step 97000:
  train loss: 3.84e-05
  test loss: 3.84e-05
  test metric: []

'train' took 21.419089 s

Correlation Coefficient: 0.8816283208071956
