/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using backend: tensorflow.compat.v1
Other supported backends: tensorflow, pytorch, jax, paddle.
paddle supports more examples now and is recommended.
WARNING:tensorflow:From /home/bogdan/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2023-11-18 11:32:10.262693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 11:32:10.296635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 11:32:10.296816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
Enable just-in-time compilation with XLA.

WARNING:tensorflow:From /home/bogdan/.local/lib/python3.8/site-packages/deepxde/nn/initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
/home/bogdan/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/fnn.py:116: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.036782 s

2023-11-18 11:32:27.591476: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-18 11:32:27.592177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 11:32:27.592358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 11:32:27.592483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 11:32:27.962171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 11:32:27.962343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 11:32:27.962474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 11:32:27.962566: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2023-11-18 11:32:27.962593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8097 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:27:00.0, compute capability: 8.6
'compile' took 0.648521 s

2023-11-18 11:32:28.207937: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
Training model...

2023-11-18 11:32:28.295927: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbd7c094040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-11-18 11:32:28.295973: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6
2023-11-18 11:32:28.314328: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-11-18 11:32:32.585092: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Step      Train loss              Test loss               Test metric
0         [5.49e+01, 3.19e-02]    [5.49e+01, 3.19e-02]    []  
1000      [2.03e+00, 1.63e-01]    [2.03e+00, 1.63e-01]    []  
2000      [1.23e+00, 1.70e-01]    [1.23e+00, 1.70e-01]    []  
3000      [7.92e-01, 1.74e-01]    [7.92e-01, 1.74e-01]    []  
4000      [5.00e-01, 1.81e-01]    [5.00e-01, 1.81e-01]    []  
5000      [3.61e-01, 1.81e-01]    [3.61e-01, 1.81e-01]    []  
6000      [2.37e-01, 1.82e-01]    [2.37e-01, 1.82e-01]    []  
7000      [2.15e-01, 1.81e-01]    [2.15e-01, 1.81e-01]    []  
8000      [1.87e-01, 1.80e-01]    [1.87e-01, 1.80e-01]    []  
9000      [1.86e-01, 1.80e-01]    [1.86e-01, 1.80e-01]    []  
10000     [9.51e-01, 1.64e-01]    [9.51e-01, 1.64e-01]    []  
11000     [1.50e-01, 1.79e-01]    [1.50e-01, 1.79e-01]    []  
12000     [1.56e-01, 1.76e-01]    [1.56e-01, 1.76e-01]    []  
13000     [1.28e-01, 1.75e-01]    [1.28e-01, 1.75e-01]    []  
14000     [1.14e-01, 1.66e-01]    [1.14e-01, 1.66e-01]    []  
15000     [1.26e-01, 1.49e-01]    [1.26e-01, 1.49e-01]    []  
16000     [1.47e-01, 6.00e-02]    [1.47e-01, 6.00e-02]    []  
17000     [8.70e-02, 5.29e-03]    [8.70e-02, 5.29e-03]    []  
18000     [6.19e-02, 1.95e-03]    [6.19e-02, 1.95e-03]    []  
19000     [1.72e-01, 1.39e-03]    [1.72e-01, 1.39e-03]    []  
20000     [2.23e-01, 8.35e-04]    [2.23e-01, 8.35e-04]    []  
21000     [5.52e-02, 6.32e-04]    [5.52e-02, 6.32e-04]    []  
22000     [6.02e-02, 4.67e-04]    [6.02e-02, 4.67e-04]    []  
23000     [5.17e-02, 3.48e-04]    [5.17e-02, 3.48e-04]    []  
24000     [5.76e-02, 2.62e-04]    [5.76e-02, 2.62e-04]    []  
25000     [4.19e-02, 2.65e-04]    [4.19e-02, 2.65e-04]    []  
26000     [4.52e-02, 1.84e-04]    [4.52e-02, 1.84e-04]    []  
27000     [4.91e-02, 2.30e-04]    [4.91e-02, 2.30e-04]    []  
28000     [3.82e-02, 2.26e-04]    [3.82e-02, 2.26e-04]    []  
29000     [4.20e-02, 1.91e-04]    [4.20e-02, 1.91e-04]    []  
30000     [4.83e-02, 1.82e-04]    [4.83e-02, 1.82e-04]    []  
31000     [5.55e-02, 2.51e-04]    [5.55e-02, 2.51e-04]    []  
32000     [5.24e-02, 4.33e-04]    [5.24e-02, 4.33e-04]    []  
33000     [3.42e-02, 1.29e-04]    [3.42e-02, 1.29e-04]    []  
34000     [4.42e-02, 1.72e-04]    [4.42e-02, 1.72e-04]    []  
35000     [3.52e-02, 1.41e-04]    [3.52e-02, 1.41e-04]    []  
36000     [4.66e-02, 1.62e-04]    [4.66e-02, 1.62e-04]    []  
37000     [3.48e-02, 1.55e-04]    [3.48e-02, 1.55e-04]    []  
38000     [3.50e-02, 1.77e-04]    [3.50e-02, 1.77e-04]    []  
39000     [3.09e-02, 2.00e-04]    [3.09e-02, 2.00e-04]    []  
40000     [3.35e-02, 1.98e-04]    [3.35e-02, 1.98e-04]    []  
41000     [4.33e-02, 1.77e-04]    [4.33e-02, 1.77e-04]    []  
42000     [3.58e-02, 1.91e-04]    [3.58e-02, 1.91e-04]    []  
43000     [3.30e-02, 1.93e-04]    [3.30e-02, 1.93e-04]    []  
44000     [2.68e-02, 2.07e-04]    [2.68e-02, 2.07e-04]    []  
45000     [4.31e-02, 1.88e-04]    [4.31e-02, 1.88e-04]    []  
46000     [5.02e-02, 2.41e-04]    [5.02e-02, 2.41e-04]    []  
47000     [4.99e-02, 2.15e-04]    [4.99e-02, 2.15e-04]    []  
48000     [2.25e-02, 1.88e-04]    [2.25e-02, 1.88e-04]    []  
49000     [3.40e-02, 2.03e-04]    [3.40e-02, 2.03e-04]    []  
50000     [3.18e-02, 2.02e-04]    [3.18e-02, 2.02e-04]    []  
51000     [3.28e-02, 2.11e-04]    [3.28e-02, 2.11e-04]    []  
52000     [3.61e-02, 2.33e-04]    [3.61e-02, 2.33e-04]    []  
53000     [3.89e-02, 2.00e-04]    [3.89e-02, 2.00e-04]    []  
54000     [5.12e-02, 2.18e-04]    [5.12e-02, 2.18e-04]    []  
55000     [2.36e-02, 2.07e-04]    [2.36e-02, 2.07e-04]    []  
56000     [2.87e-02, 2.11e-04]    [2.87e-02, 2.11e-04]    []  
57000     [1.95e-02, 2.14e-04]    [1.95e-02, 2.14e-04]    []  
58000     [2.52e-02, 2.02e-04]    [2.52e-02, 2.02e-04]    []  
59000     [3.54e-02, 2.07e-04]    [3.54e-02, 2.07e-04]    []  
60000     [3.06e-02, 1.95e-04]    [3.06e-02, 1.95e-04]    []  
61000     [3.14e-02, 2.14e-04]    [3.14e-02, 2.14e-04]    []  
62000     [6.61e-02, 2.51e-04]    [6.61e-02, 2.51e-04]    []  
63000     [2.89e-02, 2.36e-04]    [2.89e-02, 2.36e-04]    []  
64000     [1.85e-02, 1.97e-04]    [1.85e-02, 1.97e-04]    []  
65000     [2.05e-02, 2.13e-04]    [2.05e-02, 2.13e-04]    []  
66000     [4.11e-02, 2.17e-04]    [4.11e-02, 2.17e-04]    []  
67000     [2.69e-02, 2.28e-04]    [2.69e-02, 2.28e-04]    []  
68000     [2.76e-02, 2.23e-04]    [2.76e-02, 2.23e-04]    []  
69000     [3.15e-02, 2.16e-04]    [3.15e-02, 2.16e-04]    []  
70000     [2.19e-02, 1.73e-04]    [2.19e-02, 1.73e-04]    []  
71000     [2.26e-02, 1.93e-04]    [2.26e-02, 1.93e-04]    []  
72000     [3.17e-02, 1.90e-04]    [3.17e-02, 1.90e-04]    []  
73000     [5.01e-02, 2.08e-04]    [5.01e-02, 2.08e-04]    []  
74000     [3.26e-02, 1.94e-04]    [3.26e-02, 1.94e-04]    []  
75000     [2.26e-02, 2.80e-04]    [2.26e-02, 2.80e-04]    []  
76000     [1.96e-02, 1.97e-04]    [1.96e-02, 1.97e-04]    []  
77000     [3.44e-02, 1.97e-04]    [3.44e-02, 1.97e-04]    []  
78000     [1.90e-02, 2.08e-04]    [1.90e-02, 2.08e-04]    []  
79000     [2.50e-02, 2.00e-04]    [2.50e-02, 2.00e-04]    []  
80000     [1.86e-02, 2.08e-04]    [1.86e-02, 2.08e-04]    []  
81000     [3.26e-02, 1.99e-04]    [3.26e-02, 1.99e-04]    []  
82000     [2.66e-02, 2.62e-04]    [2.66e-02, 2.62e-04]    []  
83000     [1.68e-02, 2.18e-04]    [1.68e-02, 2.18e-04]    []  
84000     [5.68e-02, 1.69e-04]    [5.68e-02, 1.69e-04]    []  
85000     [1.57e-02, 2.10e-04]    [1.57e-02, 2.10e-04]    []  
86000     [2.80e-02, 2.37e-04]    [2.80e-02, 2.37e-04]    []  
87000     [2.71e-02, 1.83e-04]    [2.71e-02, 1.83e-04]    []  
88000     [3.69e-02, 1.94e-04]    [3.69e-02, 1.94e-04]    []  
89000     [5.61e-02, 2.47e-04]    [5.61e-02, 2.47e-04]    []  
90000     [2.21e-02, 2.15e-04]    [2.21e-02, 2.15e-04]    []  
91000     [2.42e-02, 2.19e-04]    [2.42e-02, 2.19e-04]    []  
92000     [3.99e-02, 2.31e-04]    [3.99e-02, 2.31e-04]    []  
93000     [3.89e-02, 2.43e-04]    [3.89e-02, 2.43e-04]    []  
94000     [2.03e-02, 2.35e-04]    [2.03e-02, 2.35e-04]    []  
95000     [2.47e-02, 2.03e-04]    [2.47e-02, 2.03e-04]    []  
96000     [3.11e-02, 2.26e-04]    [3.11e-02, 2.26e-04]    []  
97000     [2.74e-02, 2.24e-04]    [2.74e-02, 2.24e-04]    []  
98000     [2.13e-02, 2.30e-04]    [2.13e-02, 2.30e-04]    []  
99000     [3.17e-02, 1.94e-04]    [3.17e-02, 1.94e-04]    []  
100000    [4.39e-02, 2.32e-04]    [4.39e-02, 2.32e-04]    []  

Best model at step 85000:
  train loss: 1.59e-02
  test loss: 1.59e-02
  test metric: []

'train' took 2667.271146 s

Compiling model...
'compile' took 0.169765 s

2023-11-18 12:16:55.692503: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1750] (One-time warning): Not using XLA:CPU for cluster.

If you want XLA:CPU, do one of the following:

 - set the TF_XLA_FLAGS to include "--tf_xla_cpu_global_jit", or
 - set cpu_global_jit to true on this session's OptimizerOptions, or
 - use experimental_jit_scope, or
 - use tf.function(jit_compile=True).

To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a
proper command-line flag, not via TF_XLA_FLAGS).
Training model...

Step      Train loss              Test loss               Test metric
100000    [4.39e-01, 2.32e-04]    [4.39e-01, 2.32e-04]    []  
100605    [1.22e-01, 4.99e-04]    [1.22e-01, 4.99e-04]    []  

Best model at step 85000:
  train loss: 1.59e-02
  test loss: 1.59e-02
  test metric: []

'train' took 20.501796 s

Correlation Coefficient: 0.9956181716635079
