/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using backend: tensorflow.compat.v1
Other supported backends: tensorflow, pytorch, jax, paddle.
paddle supports more examples now and is recommended.
WARNING:tensorflow:From /home/bogdan/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2023-11-18 16:08:14.997395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 16:08:15.027610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 16:08:15.027790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
Enable just-in-time compilation with XLA.

WARNING:tensorflow:From /home/bogdan/.local/lib/python3.8/site-packages/deepxde/nn/initializers.py:118: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.

Compiling model...
Building feed-forward neural network...
/home/bogdan/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/fnn.py:116: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  return tf.layers.dense(
'build' took 0.036348 s

2023-11-18 16:08:29.505854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-18 16:08:29.506552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 16:08:29.506737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 16:08:29.506862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 16:08:29.880187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 16:08:29.880358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 16:08:29.880490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-11-18 16:08:29.880583: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2023-11-18 16:08:29.880611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8097 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:27:00.0, compute capability: 8.6
'compile' took 0.649609 s

2023-11-18 16:08:30.123824: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
Training model...

2023-11-18 16:08:30.208172: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f564800cc40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-11-18 16:08:30.208222: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6
2023-11-18 16:08:30.220052: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-11-18 16:08:31.633262: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Step      Train loss              Test loss               Test metric
0         [5.49e+01, 3.19e-02]    [5.49e+01, 3.19e-02]    []  
1000      [2.01e+00, 1.64e-01]    [2.01e+00, 1.64e-01]    []  
2000      [1.21e+00, 1.71e-01]    [1.21e+00, 1.71e-01]    []  
3000      [7.75e-01, 1.76e-01]    [7.75e-01, 1.76e-01]    []  
4000      [4.87e-01, 1.82e-01]    [4.87e-01, 1.82e-01]    []  
5000      [3.50e-01, 1.82e-01]    [3.50e-01, 1.82e-01]    []  
6000      [3.72e-01, 1.74e-01]    [3.72e-01, 1.74e-01]    []  
7000      [2.00e-01, 1.80e-01]    [2.00e-01, 1.80e-01]    []  
8000      [6.24e-01, 1.94e-01]    [6.24e-01, 1.94e-01]    []  
9000      [1.68e-01, 1.79e-01]    [1.68e-01, 1.79e-01]    []  
10000     [1.55e-01, 1.78e-01]    [1.55e-01, 1.78e-01]    []  
11000     [4.11e-01, 1.82e-01]    [4.11e-01, 1.82e-01]    []  
12000     [1.47e-01, 1.68e-01]    [1.47e-01, 1.68e-01]    []  
13000     [1.35e-01, 1.50e-01]    [1.35e-01, 1.50e-01]    []  
14000     [1.25e-01, 1.12e-01]    [1.25e-01, 1.12e-01]    []  
15000     [1.37e-01, 2.87e-02]    [1.37e-01, 2.87e-02]    []  
16000     [9.38e-02, 8.80e-03]    [9.38e-02, 8.80e-03]    []  
17000     [7.13e-02, 2.18e-03]    [7.13e-02, 2.18e-03]    []  
18000     [6.21e-02, 9.27e-04]    [6.21e-02, 9.27e-04]    []  
19000     [1.11e-01, 1.16e-03]    [1.11e-01, 1.16e-03]    []  
20000     [7.15e-02, 8.46e-04]    [7.15e-02, 8.46e-04]    []  
21000     [5.48e-02, 5.70e-04]    [5.48e-02, 5.70e-04]    []  
22000     [5.88e-02, 5.36e-04]    [5.88e-02, 5.36e-04]    []  
23000     [6.64e-02, 3.70e-04]    [6.64e-02, 3.70e-04]    []  
24000     [3.74e-02, 3.67e-04]    [3.74e-02, 3.67e-04]    []  
25000     [6.86e-02, 3.29e-04]    [6.86e-02, 3.29e-04]    []  
26000     [5.65e-02, 2.39e-04]    [5.65e-02, 2.39e-04]    []  
27000     [4.15e-02, 1.88e-04]    [4.15e-02, 1.88e-04]    []  
28000     [3.67e-02, 1.45e-04]    [3.67e-02, 1.45e-04]    []  
29000     [5.17e-02, 1.67e-04]    [5.17e-02, 1.67e-04]    []  
30000     [4.92e-02, 1.53e-04]    [4.92e-02, 1.53e-04]    []  
31000     [3.52e-02, 1.19e-04]    [3.52e-02, 1.19e-04]    []  
32000     [2.49e-02, 1.41e-04]    [2.49e-02, 1.41e-04]    []  
33000     [2.83e-02, 1.60e-04]    [2.83e-02, 1.60e-04]    []  
34000     [3.04e-02, 1.46e-04]    [3.04e-02, 1.46e-04]    []  
35000     [3.33e-02, 1.92e-04]    [3.33e-02, 1.92e-04]    []  
36000     [3.70e-02, 1.84e-04]    [3.70e-02, 1.84e-04]    []  
37000     [2.18e-02, 1.47e-04]    [2.18e-02, 1.47e-04]    []  
38000     [2.56e-02, 2.02e-04]    [2.56e-02, 2.02e-04]    []  
39000     [2.49e-02, 1.83e-04]    [2.49e-02, 1.83e-04]    []  
40000     [3.89e-02, 2.20e-04]    [3.89e-02, 2.20e-04]    []  
41000     [2.23e-02, 1.58e-04]    [2.23e-02, 1.58e-04]    []  
42000     [5.16e-02, 1.54e-04]    [5.16e-02, 1.54e-04]    []  
43000     [2.41e-02, 1.51e-04]    [2.41e-02, 1.51e-04]    []  
44000     [1.91e-02, 1.54e-04]    [1.91e-02, 1.54e-04]    []  
45000     [3.50e-02, 1.80e-04]    [3.50e-02, 1.80e-04]    []  
46000     [5.14e-02, 1.60e-04]    [5.14e-02, 1.60e-04]    []  
47000     [1.67e-02, 1.74e-04]    [1.67e-02, 1.74e-04]    []  
48000     [2.06e-02, 2.44e-04]    [2.06e-02, 2.44e-04]    []  
49000     [3.31e-02, 1.52e-04]    [3.31e-02, 1.52e-04]    []  
50000     [3.22e-02, 1.78e-04]    [3.22e-02, 1.78e-04]    []  
51000     [4.85e-02, 1.48e-04]    [4.85e-02, 1.48e-04]    []  
52000     [4.10e-02, 1.34e-04]    [4.10e-02, 1.34e-04]    []  
53000     [1.57e-02, 1.38e-04]    [1.57e-02, 1.38e-04]    []  
54000     [2.64e-02, 1.41e-04]    [2.64e-02, 1.41e-04]    []  
55000     [3.51e-02, 1.46e-04]    [3.51e-02, 1.46e-04]    []  
56000     [1.69e-02, 1.81e-04]    [1.69e-02, 1.81e-04]    []  
57000     [1.59e-02, 1.36e-04]    [1.59e-02, 1.36e-04]    []  
58000     [3.85e-02, 1.63e-04]    [3.85e-02, 1.63e-04]    []  
59000     [1.57e-02, 1.59e-04]    [1.57e-02, 1.59e-04]    []  
60000     [4.95e-02, 1.48e-04]    [4.95e-02, 1.48e-04]    []  
61000     [6.31e-02, 1.55e-04]    [6.31e-02, 1.55e-04]    []  
62000     [4.21e-02, 1.80e-04]    [4.21e-02, 1.80e-04]    []  
63000     [3.97e-02, 1.98e-04]    [3.97e-02, 1.98e-04]    []  
64000     [1.91e-02, 1.55e-04]    [1.91e-02, 1.55e-04]    []  
65000     [3.71e-02, 1.57e-04]    [3.71e-02, 1.57e-04]    []  
66000     [2.08e-02, 1.58e-04]    [2.08e-02, 1.58e-04]    []  
67000     [4.01e-02, 1.99e-04]    [4.01e-02, 1.99e-04]    []  
68000     [1.58e-02, 1.82e-04]    [1.58e-02, 1.82e-04]    []  
69000     [2.05e-02, 1.69e-04]    [2.05e-02, 1.69e-04]    []  
70000     [1.47e-02, 1.98e-04]    [1.47e-02, 1.98e-04]    []  
71000     [5.32e-02, 1.62e-04]    [5.32e-02, 1.62e-04]    []  
72000     [1.53e-02, 1.82e-04]    [1.53e-02, 1.82e-04]    []  
73000     [2.98e-02, 1.62e-04]    [2.98e-02, 1.62e-04]    []  
74000     [1.90e-02, 1.61e-04]    [1.90e-02, 1.61e-04]    []  
75000     [4.50e-02, 2.16e-04]    [4.50e-02, 2.16e-04]    []  
76000     [2.33e-02, 1.51e-04]    [2.33e-02, 1.51e-04]    []  
77000     [2.31e-02, 1.58e-04]    [2.31e-02, 1.58e-04]    []  
78000     [2.14e-02, 2.09e-04]    [2.14e-02, 2.09e-04]    []  
79000     [1.57e-02, 1.62e-04]    [1.57e-02, 1.62e-04]    []  
80000     [1.43e-02, 1.95e-04]    [1.43e-02, 1.95e-04]    []  
81000     [3.92e-02, 1.62e-04]    [3.92e-02, 1.62e-04]    []  
82000     [1.50e-02, 1.70e-04]    [1.50e-02, 1.70e-04]    []  
83000     [1.84e-02, 1.86e-04]    [1.84e-02, 1.86e-04]    []  
84000     [4.48e-02, 2.07e-04]    [4.48e-02, 2.07e-04]    []  
85000     [2.21e-02, 1.45e-04]    [2.21e-02, 1.45e-04]    []  
86000     [3.68e-02, 1.81e-04]    [3.68e-02, 1.81e-04]    []  
87000     [1.52e-02, 1.95e-04]    [1.52e-02, 1.95e-04]    []  
88000     [1.45e-02, 1.64e-04]    [1.45e-02, 1.64e-04]    []  
89000     [2.58e-02, 1.89e-04]    [2.58e-02, 1.89e-04]    []  
90000     [3.54e-02, 1.57e-04]    [3.54e-02, 1.57e-04]    []  
91000     [1.95e-02, 1.75e-04]    [1.95e-02, 1.75e-04]    []  
92000     [1.58e-02, 1.85e-04]    [1.58e-02, 1.85e-04]    []  
93000     [1.85e-02, 1.76e-04]    [1.85e-02, 1.76e-04]    []  
94000     [3.12e-02, 1.86e-04]    [3.12e-02, 1.86e-04]    []  
95000     [3.60e-02, 1.76e-04]    [3.60e-02, 1.76e-04]    []  
96000     [2.77e-02, 1.75e-04]    [2.77e-02, 1.75e-04]    []  
97000     [3.34e-02, 2.28e-04]    [3.34e-02, 2.28e-04]    []  
98000     [1.45e-02, 1.85e-04]    [1.45e-02, 1.85e-04]    []  
99000     [1.62e-02, 2.34e-04]    [1.62e-02, 2.34e-04]    []  
100000    [1.61e-02, 1.81e-04]    [1.61e-02, 1.81e-04]    []  

Best model at step 80000:
  train loss: 1.45e-02
  test loss: 1.45e-02
  test metric: []

'train' took 348.453290 s

Compiling model...
'compile' took 0.170688 s

2023-11-18 16:14:18.791360: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1750] (One-time warning): Not using XLA:CPU for cluster.

If you want XLA:CPU, do one of the following:

 - set the TF_XLA_FLAGS to include "--tf_xla_cpu_global_jit", or
 - set cpu_global_jit to true on this session's OptimizerOptions, or
 - use experimental_jit_scope, or
 - use tf.function(jit_compile=True).

To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a
proper command-line flag, not via TF_XLA_FLAGS).
Training model...

Step      Train loss              Test loss               Test metric
100000    [1.61e-01, 1.81e-04]    [1.61e-01, 1.81e-04]    []  
100187    [1.17e-01, 2.66e-04]    [1.17e-01, 2.66e-04]    []  

Best model at step 80000:
  train loss: 1.45e-02
  test loss: 1.45e-02
  test metric: []

'train' took 2.649203 s

Correlation Coefficient: 0.9956413879531814
